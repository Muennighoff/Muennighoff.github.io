---
permalink: /
title: ""
excerpt: ""
author_profile: true
no_meta: true
redirect_from: 
  - /about/
  - /about.html
---

I research large language models (LLMs) as a PhD student at [Stanford](https://www.stanford.edu/). I'm fortunate to be collaborating with [Contextual AI](https://contextual.ai/) & [Ai2](https://allenai.org/). I did my Bachelor's at [Peking University](https://english.pku.edu.cn/).

If you want to work/research with me, I recommend contributing to [MTEB](https://github.com/embeddings-benchmark/mteb/issues). We are a community building the go-to place for everything related to embeddings with 200K active monthly users on [our leaderboard](https://huggingface.co/spaces/mteb/leaderboard) and regular publications you can co-author.

If you have questions about any papers I've co-authored, please open a GitHub issue on the respective repository :)

### Research

I research making LLMs more useful, e.g., by improving their capabilities. Below are some areas I've been working on with amazing collaborators & some papers I've had the chance to be involved in (see [Google Scholar](https://scholar.google.com/citations?user=Me0IoRMAAAAJ&hl=en) for all papers):
- **LLM pretraining:** I want to better understand how models scale & help us build more capable base models
  - [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264) (NeurIPS 2023, Oral & Outstanding Paper Runner-Up Award)
  - [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) & [OLMo](https://arxiv.org/abs/2402.00838) (ACL 2024, Best Theme Paper Award)
- **LLM alignment/instruction tuning:** Models need to precisely follow instructions to be truly useful
  - [Crosslingual generalization through multitask finetuning](https://arxiv.org/abs/2211.01786) (ACL 2022, [Most influential ACL paper #2](https://www.paperdigest.org/2024/05/most-influential-acl-papers-2024-05/))
  - [Octopack: Instruction tuning code large language models](https://arxiv.org/abs/2308.07124) (ICLR 2024, Spotlight)
- **Embeddings/Retrieval:** For LLMs to be useful, they often need to rely on search/retrieval, usually powered by embeddings
  - [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316) (EACL 2022, [>2M software downloads](https://www.pepy.tech/projects/mteb?versions=*))
  - [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) (ICLR 2024 AGI Workshop, Best Paper Award)

### Selected honors

- 2024: Stanford School of Engineering Fellowship; [ACL Best Theme Paper Award, Best Paper Award, Best Resource Paper Award](https://2024.aclweb.org/program/best_papers/); [ICLR AGI Workshop Best Paper Award](https://agiworkshop.github.io/2024/schedule/)
- 2023: [NeurIPS Outstanding Paper Runner-Up Award](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)
- 2020: [Meta AI Hateful Memes Challenge, 2nd place/3300+](https://ai.meta.com/blog/hateful-memes-challenge-winners/)
- 2019: JLPTN1
- 2018-2022: Peking University Full Scholarship

### Misc unsorted

- Apart from AI, I'm very interested in health (🏊🎾🏃🌸)
- I've worked in Chinese, English, Japanese, French & German, but I don't have enough parameters, so I forgot much of the last two 😅
- As a kid I worked as a voice-over artist dubbing German voices for Peter Pan (Disney), Pokemon, Game Of Thrones (HBO), Dracula (NBC) & others ([sample](https://www.audible.de/pd/Gortimer-Gibbon-Mein-Leben-in-der-Normal-Street-Die-komplette-1-Staffel-Hoerbuch/B01LY8AAZP?overrideBaseCountry=true&ipRedirectOverride=true&ref_pageloadid=not_applicable&pageLoadId=pCyLZcePVMNX3kCH&creativeId=292d6343-f11b-4bbe-a8a5-d4b7272abf61)) 😁
