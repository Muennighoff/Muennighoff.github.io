---
permalink: /
title: ""
excerpt: ""
author_profile: true
no_meta: true
redirect_from: 
  - /about/
  - /about.html
---

I research large language models (LLMs) as a 1st year PhD at [Stanford](https://www.stanford.edu/) rotating with [Percy Liang](https://cs.stanford.edu/~pliang/) & [Tatsunori Hashimoto](https://thashim.github.io/). I'm fortunate to be collaborating with [Contextual AI](https://contextual.ai/) & [Ai2](https://allenai.org/). I did my Bachelor's at [Peking University](https://english.pku.edu.cn/).

Feel free to reach out, especially for doing research together. Iâ€™m also happy to advise self-motivated people on a research project - I have many ideas I think could be impactful & we can work on together :)

### Research

I research making LLMs more useful, e.g. by improving their capabilities. Below are some areas I've been working on with amazing collaborators & some papers I've had the chance to be involved in (see [Google Scholar](https://scholar.google.com/citations?user=Me0IoRMAAAAJ&hl=en) for all papers):
- **LLM pretraining:** I want to better understand how models scale & help us build more capable base models
  - [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264) (NeurIPS 2023, Oral & Outstanding Paper Runner-Up Award)
  - [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) & [OLMo](https://arxiv.org/abs/2402.00838) (ACL 2024, Best Theme Paper Award)
- **LLM alignment/instruction tuning:** Models need to precisely follow instructions to be truly useful
  - [Crosslingual generalization through multitask finetuning](https://arxiv.org/abs/2211.01786) (ACL 2022, [Most influential ACL paper #2](https://www.paperdigest.org/2024/05/most-influential-acl-papers-2024-05/))
  - [Octopack: Instruction tuning code large language models](https://arxiv.org/abs/2308.07124) (ICLR 2024, Spotlight)
- **Embeddings/Retrieval:** For LLMs to be useful, they often need to rely on search/retrieval, usually powered by embeddings
  - [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316) (EACL 2022, [>1M software downloads](https://www.pepy.tech/projects/mteb?versions=*))
  - [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) (ICLR 2024 AGI Workshop, Best Paper Award)

### Selected honors

- 2024: Stanford School of Engineering Fellowship; ACL [Best Theme Paper Award](https://x.com/aclmeeting/status/1823664612677705762), [Best Paper Award](https://x.com/aclmeeting/status/1823664612207743110), [Best Resource Paper Award](https://x.com/aclmeeting/status/1823664612577051026); [ICLR AGI Workshop Best Paper Award](https://x.com/chrmanning/status/1789197942403813870)
- 2023: [NeurIPS Outstanding Paper Runner-Up Award](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)
- 2020: [Meta AI Hateful Memes Challenge, 2nd place/3300+](https://ai.meta.com/blog/hateful-memes-challenge-winners/)
- 2019: JLPTN1
- 2018-2022: Peking University Full Scholarship

### Misc unsorted

- Apart from AI, I'm very interested in health (ğŸŠğŸ¾ğŸƒğŸŒ¸)
- I've worked in Chinese, English, Japanese, French & German, but I don't have enough parameters so I forgot much of the last two ğŸ˜…
- As a kid I worked as a voice-over artist dubbing German voices for Peter Pan (Disney), Pokemon (Pokemon Company), Game Of Thrones (HBO), Dracula (NBC) & others ([sample](https://www.audible.de/pd/Gortimer-Gibbon-Mein-Leben-in-der-Normal-Street-Die-komplette-1-Staffel-Hoerbuch/B01LY8AAZP?overrideBaseCountry=true&ipRedirectOverride=true&ref_pageloadid=not_applicable&pageLoadId=pCyLZcePVMNX3kCH&creativeId=292d6343-f11b-4bbe-a8a5-d4b7272abf61)) ğŸ˜
