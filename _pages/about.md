---
permalink: /
title: ""
excerpt: ""
author_profile: true
no_meta: true
redirect_from: 
  - /about/
  - /about.html
---

I research AI/LLMs as a PhD student at [Stanford](https://www.stanford.edu/). I did my Bachelor's at [Peking University](https://english.pku.edu.cn/).

For working/researching together, I recommend contributing to [MTEB](https://github.com/embeddings-benchmark/mteb/issues). We're a community building the go-to place for everything embeddings with 200K active monthly users on [our leaderboard](https://huggingface.co/spaces/mteb/leaderboard) & regular publications you can co-author.

For questions about anything I've co-authored, please open a GitHub issue on its repository :)

### Research

I research making LLMs more useful/capable. Below are some areas I've been working on with amazing collaborators & some papers I've had the chance to be involved in (see [Google Scholar](https://scholar.google.com/citations?user=Me0IoRMAAAAJ&hl=en) for more):
- **LLM pretraining:** To build more capable base models
  - [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264) (NeurIPS 2023, Outstanding Paper Runner-Up Award)
  - [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) (ICLR 2025 Oral)
- **LLM posttraining:** To make models follow instructions, reason, etc
  - [Crosslingual generalization through multitask finetuning](https://arxiv.org/abs/2211.01786) (ACL 2022, [Most influential ACL paper #2](https://www.paperdigest.org/2024/05/most-influential-acl-papers-2024-05/))
  - [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393) (ICLR 2025 Reasoning Workshop, Best Paper Award)
- **Embeddings/Retrieval:** For LLMs to be useful, they often need to rely on search/retrieval, usually powered by embeddings
  - [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316) (EACL 2022, [6M software downloads](https://www.pepy.tech/projects/mteb?versions=*))
  - [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) (ICLR 2024 AGI Workshop, Best Paper Award)

### Select honors

- 2025: [Knight-Hennessy Scholarship](https://knight-hennessy.stanford.edu/people/niklas-muennighoff), ICLR Reasoning and Planning Workshop Best Paper Award, [CVPR Best Paper Honorable Mention](https://cvpr.thecvf.com/Conferences/2025/News/Awards_Press)
- 2024: Stanford School of Engineering Fellowship; [ACL Best Theme Paper Award, Best Paper Award, Best Resource Paper Award](https://2024.aclweb.org/program/best_papers/); [ICLR AGI Workshop Best Paper Award](https://agiworkshop.github.io/2024/schedule/)
- 2023: [NeurIPS Outstanding Paper Runner-Up Award](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)
- 2020: [Meta AI Hateful Memes Challenge, 2nd place/3300+](https://ai.meta.com/blog/hateful-memes-challenge-winners/)
- 2019: JLPTN1
- 2018-2022: Peking University Full Scholarship

### Misc

- Apart from AI, I'm very interested in health (üèäüéæüèêüèÉüå∏)
- I've worked in Chinese, English, Japanese, French & German, but I don't have enough parameters, so I forgot much of the last two üòÖ
- As a kid I worked as a voice-over artist dubbing German voices for Peter Pan (Disney), Pokemon, Game Of Thrones (HBO), Dracula (NBC) & others ([sample](https://www.audible.de/pd/Gortimer-Gibbon-Mein-Leben-in-der-Normal-Street-Die-komplette-1-Staffel-Hoerbuch/B01LY8AAZP?overrideBaseCountry=true&ipRedirectOverride=true&ref_pageloadid=not_applicable&pageLoadId=pCyLZcePVMNX3kCH&creativeId=292d6343-f11b-4bbe-a8a5-d4b7272abf61)) üòÅ
