---
permalink: /
title: ""
excerpt: ""
author_profile: true
no_meta: true
redirect_from: 
  - /about/
  - /about.html
---

I research **large language models (LLMs)** at Contextual AI & starting in September as a PhD student at Stanford University. I did my Bachelor's at Peking University. I've worked in Chinese, English, Japanese, French & German, but I don't have enough parameters so I forgot much of the last two 😅

My research focuses on advancing LLM capabilities, where I've had the chance to work with amazing collaborators on:

**🧙🏻‍♂️ Scaling:** Scaling up LLMs is critical to improve them. I study how it works ([data-constrained scaling laws](https://arxiv.org/abs/2305.16264), [overtraining](https://arxiv.org/abs/2403.08540)) & apply it to create open LLMs ([OLMo](https://arxiv.org/abs/2402.00838), [StarCoder](https://arxiv.org/abs/2305.06161), [BLOOM](https://arxiv.org/abs/2211.05100)).

**🫡 Alignment/Instruction-following:** To make models maximally useful, they need to closely follow instructions. I research improving it in various settings, e.g. multilingualism ([BLOOMZ/mT0](https://arxiv.org/abs/2211.01786)), coding ([OctoPack](https://arxiv.org/abs/2308.07124)), or embedding + generative ([GRIT](https://arxiv.org/abs/2402.09906)).

**🤔 Other:** I've also had the chance to work on the largest text embedding benchmark ([MTEB](https://arxiv.org/abs/2210.07316)) & build multimodal models winning 2nd in Meta's Hateful Memes Challenge ([Blog](https://ai.facebook.com/blog/hateful-memes-challenge-winners/)).

I love this field & am very optimistic about the future of AI ❤️ Apart from AI, I am very interested in health (🏊, 🎾, 🏃, 🌸).

Feel free to reach out, I'm especially interested in hearing about possible research collaborations! I'm also happy to advise self-motivated students on a research project - I have many ideas that I think could be very impactful & we could work on together if interested (Note that I won't be able to pay you - only advise & provide compute).
